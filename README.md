# Supertools

<p align="center">
  <img src="https://raw.githubusercontent.com/bxxf/supertools/refs/heads/main/assets/banner.svg" alt="Supertools - Let LLMs write code that calls your tools" width="100%">
</p>

> **ðŸš§ Work in Progress** â€” This project is under active development. Contributions are welcome, especially for adding support for other AI providers (OpenAI, Vercel AI SDK, etc.)!

<p align="center">
  <a href="#quick-start">Quick Start</a> â€¢
  <a href="#how-it-works">How It Works</a> â€¢
  <a href="#api">API</a> â€¢
  <a href="#architecture">Architecture</a> â€¢
  <a href="#roadmap">Roadmap</a>
</p>

---

> Inspired by [Anthropic's Programmatic Tool Calling](https://platform.claude.com/docs/en/agents-and-tools/tool-use/programmatic-tool-calling) â€” the LLM writes code that orchestrates tools, instead of calling them one by one.

## The Problem

Traditional tool calling has limitations:

- **Loops require enumeration** â€” querying 50 states means 50 explicit tool calls
- **Results stay in context** â€” all tool outputs consume tokens on every round-trip
- **Processing needs LLM** â€” filtering, aggregating, or transforming data requires another LLM call

## The Solution

Supertools lets the LLM write code that runs in a sandbox:

```
User Request â†’ LLM generates code â†’ Sandbox executes â†’ Result
                                         â”‚
                              for (state of states) {
                                await query_db(state)
                              }
                              // Process locally
                              return topResults
```

- **Loops are native** â€” the LLM writes a `for` loop, not 50 tool calls
- **Processing is free** â€” filtering/aggregation runs in sandbox, not LLM
- **Only final result** â€” intermediate data never hits the LLM context

## Quick Start

```bash
bun add @supertools-ai/core @anthropic-ai/sdk e2b
```

```bash
# .env
ANTHROPIC_API_KEY=your-key  # console.anthropic.com
E2B_API_KEY=your-key        # e2b.dev
```

### 1. Define a tool

```typescript
import { defineTool, z } from '@supertools-ai/core';

const orders = [
  { id: 1, customer: 'Alice', total: 150, status: 'completed' },
  { id: 2, customer: 'Bob', total: 75, status: 'pending' },
];

const getOrders = defineTool({
  name: 'getOrders',
  description: 'Get orders, optionally filtered by status',
  parameters: z.object({
    status: z.enum(['pending', 'completed']).optional(),
  }),
  execute: async ({ status }) =>
    status ? orders.filter(o => o.status === status) : orders,
});
```

### 2. Wrap your client

```typescript
import { supertools, SANDBOX_TEMPLATE } from '@supertools-ai/core';
import { Sandbox } from 'e2b';
import Anthropic from '@anthropic-ai/sdk';

const sandbox = await Sandbox.create(SANDBOX_TEMPLATE);

const client = supertools(new Anthropic(), {
  tools: [getOrders],
  sandbox,
  onEvent: (e) => {
    if (e.type === 'result') console.log('Result:', e.data);
  },
});
```

### 3. Use it like normal

```typescript
await client.messages.create({
  model: 'claude-sonnet-4-5',
  max_tokens: 1024,
  messages: [{
    role: 'user',
    content: 'Get completed orders and calculate total revenue',
  }],
});

await sandbox.kill(); // Clean up when done
```

**What happens:** The LLM writes code that calls `getOrders()`, filters results, and calculates the sum â€” all in one API call.

## How It Works

When you ask: *"Query sales for all 50 states, find top 5, email a report"*

### Traditional Tool Calling

The LLM calls tools one by one, each requiring an API round-trip:

```
User: "Query sales for all 50 states..."
  â†“
LLM â†’ tool_use: query_database({state: 'AL'})  â†’ API call #1
  â†“ result goes back to LLM context
LLM â†’ tool_use: query_database({state: 'AK'})  â†’ API call #2
  â†“ result goes back to LLM context
... 48 more API calls, all results accumulating in context ...
  â†“
LLM â†’ tool_use: send_email({...})              â†’ API call #51
  â†“
LLM: "Done! Here's your report..."             â†’ API call #52
```

**Problems:** 52 API calls, all 50 query results in LLM context (expensive), slow.

### With Supertools

The LLM generates code once, which runs in a sandbox:

```
User: "Query sales for all 50 states..."
  â†“
LLM generates JavaScript                       â†’ API call #1
  â†“
Sandbox executes code:
  â”œâ”€â”€ query_database('AL') â”€â”
  â”œâ”€â”€ query_database('AK')  â”œâ”€â”€ WebSocket (fast, parallel)
  â”œâ”€â”€ ... 48 more ...       â”‚
  â”œâ”€â”€ send_email()         â”€â”˜
  â””â”€â”€ return { topStates, reportSent }
  â†“
Result returned to your app                    â†’ Done!
```

**The generated code:**

```javascript
const states = ['AL', 'AK', 'AZ', /* ... all 50 */];
const results = {};

for (const state of states) {
  const data = await mcp.call('host.query_database', {
    sql: `SELECT SUM(revenue) FROM sales WHERE state = '${state}'`
  });
  results[state] = data[0].sum;
}

const top5 = Object.entries(results)
  .sort((a, b) => b[1] - a[1])
  .slice(0, 5);

await mcp.call('host.send_email', {
  to: 'ceo@company.com',
  subject: 'Top 5 States Report',
  body: top5.map(([state, rev]) => `${state}: $${rev}`).join('\n')
});

return { topStates: top5, reportSent: true };
```

**Result:** 1 API call, 51 tool executions via WebSocket, data processing in sandbox (free), only final result returned.

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         Your Application                          â”‚
â”‚                                                                   â”‚
â”‚  const client = supertools(new Anthropic(), { tools, sandbox });  â”‚
â”‚  const response = await client.messages.create({...});            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                                  â–¼
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚     Supertools Wrapper     â”‚
                   â”‚   (intercepts SDK calls)   â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚ LLM generates JavaScript
                                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        E2B Cloud Sandbox                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                       Generated Code                        â”‚  â”‚
â”‚  â”‚                                                             â”‚  â”‚
â”‚  â”‚   const [orders, users] = await Promise.all([               â”‚  â”‚
â”‚  â”‚     mcp.call('host.get_orders', {}),                        â”‚  â”‚
â”‚  â”‚     mcp.call('host.get_users', {})                          â”‚  â”‚
â”‚  â”‚   ]);                                                       â”‚  â”‚
â”‚  â”‚   return { orders, users };                                 â”‚  â”‚
â”‚  â”‚                                                             â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                               â”‚ tool calls via WebSocket          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                    Relay Server (Bun)                       â”‚  â”‚
â”‚  â”‚                  WebSocket bridge to host                   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚ WebSocket (authenticated)
                                â–¼
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚        Relay Client        â”‚
                   â”‚    (runs on your host)     â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                                  â–¼
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚         Your Tools         â”‚
                   â”‚   get_orders, get_users    â”‚
                   â”‚      (execute locally)     â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Step by step:**

1. You wrap your SDK client with `supertools()`
2. When you call `client.messages.create()`, supertools intercepts it
3. The LLM generates JavaScript code that uses `mcp.call()` for tools
4. Code runs in an isolated E2B sandbox (secure, no host access)
5. Tool calls relay back to your machine via WebSocket
6. Your tools execute locally with full access to your systems
7. Results flow back to the sandbox, code continues executing
8. Final output returns in the expected SDK response format

**Security:**

- LLM-generated code runs in isolated cloud containers
- Your tools run locally â€” the sandbox never has direct access
- WebSocket authenticated with cryptographically secure tokens
- Tokens are single-use and expire with the sandbox

> **Note:** The Relay Server runs inside the pre-built `SANDBOX_TEMPLATE`. The Relay Client is included in `@supertools-ai/core` and runs on your host.

## MCP Under the Hood

Supertools uses the [Model Context Protocol (MCP)](https://modelcontextprotocol.io) internally as a unified interface for tool communication. Here's why and how:

### Why MCP?

MCP provides a standardized way to expose tools to LLMs. Instead of inventing a custom protocol, Supertools converts your Zod-defined tools into MCP format:

```
Your Tool (Zod)  â†’  MCP Tool Definition  â†’  LLM sees it  â†’  Generates mcp.call()
```

### How tools are exposed

When you define a tool with `defineTool()`, it gets converted to MCP format with:
- **Name**: `host.your_tool_name` (prefixed with server name)
- **Description**: Your tool's description
- **Input schema**: JSON Schema derived from your Zod parameters
- **Output schema**: JSON Schema from your `returns` Zod schema (if provided)

The LLM then generates code using the `mcp.call()` pattern:

```javascript
// Your tool: getOrders
// Becomes: mcp.call('host.get_orders', { status: 'completed' })

const [orders, users] = await Promise.all([
  mcp.call('host.get_orders', { status: 'completed' }),
  mcp.call('host.get_users', {})
]);
```

### Host vs Local tools

Tools can run in two places:

| Type | Prefix | Where it runs | Use case |
|------|--------|---------------|----------|
| **Host** | `host.` | Your machine | DB queries, API calls, secrets |
| **Local** | `local.` | In sandbox | Pure computation, data transforms |

```typescript
// Host tool - runs on your machine (default)
const queryDb = defineTool({
  name: 'queryDb',
  execute: async ({ sql }) => db.query(sql), // Has access to your DB
});

// Local tool - runs in sandbox (no network round-trip)
const calculateStats = defineTool({
  name: 'calculateStats',
  local: true,  // â† This makes it local
  execute: async ({ values }) => ({
    sum: values.reduce((a, b) => a + b, 0),
    mean: values.reduce((a, b) => a + b, 0) / values.length,
  }),
});
```

Local tools are faster because they don't need a WebSocket round-trip back to your host. Use them for pure computation when all data is already in the sandbox.

## Why Supertools?

<p align="center">
  <img src="https://raw.githubusercontent.com/bxxf/supertools/refs/heads/main/assets/benchmark.svg" alt="Benchmark Results" width="100%">
</p>

The benchmark compares three approaches on the same model (Claude Sonnet 4.5):
- **Native**: Traditional tool calling with LLM round-trips
- **Anthropic Beta**: Anthropic's `code_execution` beta feature
- **Supertools**: Code generation with E2B sandbox execution

> **Note on Anthropic Beta results:** While the `allowed_callers` feature works (tools are called from within the Python code), each tool call still requires a full API round-trip. For N tool calls, you need N+1 API requests - the code execution pauses, returns to your server, you provide the result, and it continues. The only savings are that tool results don't inflate Claude's context. In contrast, Supertools makes 1 API call total - the generated code runs in the sandbox and calls tools via WebSocket without additional API round-trips. This explains the significant performance difference.

> **Note:** Supertools returns raw JSON data, not natural language. The LLM generates code but never sees the execution results. This is ideal for data pipelines and batch operations, but for chatbots needing conversational responses, consider traditional tool calling or add a summarization step.

## API

### `supertools(client, config)`

Wrap any supported LLM SDK client with programmatic tool calling.

```typescript
import { supertools, defineTool, z, SANDBOX_TEMPLATE } from '@supertools-ai/core';
import { Sandbox } from 'e2b';
import Anthropic from '@anthropic-ai/sdk';

const sandbox = await Sandbox.create(SANDBOX_TEMPLATE);
const client = supertools(new Anthropic(), {
  // Required
  tools: [defineTool({ name, description, parameters, execute })],
  sandbox,  // E2B sandbox instance

  // Optional
  debug: false,        // Enable debug logging
  instructions: '...', // Additional instructions for the LLM
  onEvent: (event) => {
    // Available event types:
    // - 'code_generated': LLM generated the code
    // - 'sandbox_ready': Sandbox connection established
    // - 'tool_call': Tool invoked (includes tool name and args)
    // - 'tool_result': Tool completed (includes result and durationMs)
    // - 'tool_error': Tool execution failed
    // - 'result': Final execution result (includes data)
    // - 'execution_error': Sandbox execution failed (includes error message)
    // - 'complete': Execution finished (success or error)
    if (event.type === 'tool_call') console.log(`Calling ${event.tool}...`);
    if (event.type === 'tool_result') console.log(`${event.tool} done in ${event.durationMs}ms`);
    if (event.type === 'result') console.log('Result:', event.data);
    if (event.type === 'execution_error') console.log('Error:', event.error);
  },
});

// Use exactly like the original SDK
const response = await client.messages.create({
  model: 'claude-haiku-4-5',
  max_tokens: 1024,
  messages: [{ role: 'user', content: 'Your request here' }],
});
```

**Supported SDKs:**
- âœ… Anthropic SDK (`@anthropic-ai/sdk`)
- â³ OpenAI SDK â€” [contributions welcome](https://github.com/anthropics/supertools)
- â³ Vercel AI SDK â€” [contributions welcome](https://github.com/anthropics/supertools)
- â³ Mastra AI â€” [contributions welcome](https://github.com/anthropics/supertools)

### `defineTool(config)`

```typescript
const tool = defineTool({
  name: 'searchUsers',           // Must match /^[a-zA-Z][a-zA-Z0-9_]*$/
  description: 'Search users',   // Used in LLM prompt (min 5 chars)
  parameters: z.object({         // Zod schema for inputs
    query: z.string(),
    limit: z.number().optional().default(10),
  }),
  returns: z.array(UserSchema),  // Optional: Zod schema for return type (improves LLM accuracy)
  execute: async (params) => {   // Your implementation
    return db.users.search(params);
  },
});
// Note: Tool names are converted to snake_case in sandbox code
// e.g., 'searchUsers' becomes 'search_users' when called

// Local tools run entirely in the sandbox (no network round-trip)
// Use for pure computation when all data is already available
const calculateStats = defineTool({
  name: 'calculateStats',
  description: 'Calculate statistics for numbers',
  parameters: z.object({ values: z.array(z.number()) }),
  returns: z.object({ mean: z.number(), sum: z.number() }),
  local: true,  // Runs in sandbox, not on host
  execute: async ({ values }) => ({
    mean: values.reduce((a, b) => a + b, 0) / values.length,
    sum: values.reduce((a, b) => a + b, 0),
  }),
});
```

### Advanced: Low-level Executor

For more control, use the executor directly:

```typescript
import { createExecutor, defineTool, SANDBOX_TEMPLATE } from '@supertools-ai/core';
import { Sandbox } from 'e2b';

// Create your own LLM adapter
const myAdapter = {
  async generateCode(request: string, systemPrompt: string) {
    // Call your LLM
    return { code: '...', rawResponse: '...' };
  },
};

const sandbox = await Sandbox.create(SANDBOX_TEMPLATE);
const executor = createExecutor({
  llm: myAdapter,
  tools: [/* your tools */],
  sandbox,
});

const result = await executor.run('Your natural language request');
console.log(result.code);           // Generated JavaScript
console.log(result.result.output);  // stdout from execution
```

## When to Use

**Use Supertools when:**
- Calling 3+ tools in sequence
- Processing data (filter/aggregate before returning)
- Parallel operations (query 50 endpoints at once)
- Complex logic (loops, conditionals, early exit)

**Use traditional tool calling when:**
- Single tool calls
- User needs to approve each step
- Tools have dangerous side effects

## Roadmap

**Coming Soon:**
- [x] Publish npm package (`@supertools-ai/core`)
- [x] Publish E2B sandbox template for zero-config setup
- [ ] Support tools from remote MCP servers ie. from the E2B MCP Gateway

**Providers:**
- [x] Anthropic SDK
- [ ] OpenAI SDK
- [ ] Vercel AI SDK
- [ ] Mastra AI

**Future:**
- [ ] Alternative sandbox providers (??)
- [ ] Python SDK (1:1 API parity)

## Requirements

- Node.js 18+ or [Bun](https://bun.sh)
- [E2B](https://e2b.dev) API key (set `E2B_API_KEY` env var)
- [Anthropic](https://anthropic.com) API key (set `ANTHROPIC_API_KEY` env var)

## License

MIT

---

<p align="center">
  Secure sandboxing powered by <a href="https://e2b.dev">E2B</a>
</p>
